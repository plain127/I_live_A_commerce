{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수: 40200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 3sent 폴더가 포함된 상위 경로 설정\n",
    "base_dir = r'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1'  # 데이터가 저장된 상위 경로\n",
    "categories = ['c_event', 'culture', 'enter', 'fm_drama', 'fs_drama', 'history']\n",
    "\n",
    "data_list = []  # 본문 데이터를 저장할 리스트\n",
    "\n",
    "# 각 카테고리의 3sent 폴더에서 데이터 추출\n",
    "for category in categories:\n",
    "    three_sent_dir = os.path.join(base_dir, category, '20per')  # 3sent 폴더 경로\n",
    "\n",
    "    # 3sent 폴더의 모든 파일 처리\n",
    "    for filename in os.listdir(three_sent_dir):\n",
    "        file_path = os.path.join(three_sent_dir, filename)\n",
    "\n",
    "        # JSON 파일 읽기\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            # 본문 데이터 추출\n",
    "            text = data.get('Meta', {}).get('passage', '')\n",
    "\n",
    "            # 본문이 존재하면 저장\n",
    "            if text:\n",
    "                data_list.append({'category': category, 'text': text})\n",
    "\n",
    "# 데이터프레임으로 변환 및 저장\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('20per_data.csv', index=False, encoding='utf-8')\n",
    "print(f\"총 데이터 개수: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: transformers in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: pandas in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# KoBART 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
    "\n",
    "# 학습 데이터 로드\n",
    "data = pd.read_csv('20per_data.csv')\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class KoBARTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, 'text']  # 본문 데이터\n",
    "\n",
    "        # 입력 텍스트와 출력 텍스트 생성\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_len, truncation=True, return_tensors='pt', padding='max_length'\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            text, max_length=self.max_len, truncation=True, return_tensors='pt', padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "dataset = KoBARTDataset(tokenizer, data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# KoBART 모델 로드\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 설정\n",
    "epochs = 3  # 학습 에포크 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5025/5025 [11:06<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.015812587825127063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5025/5025 [11:07<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0016382119948438552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5025/5025 [11:07<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0014195657006051653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 모델과 데이터를 GPU로 전송\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        # 배치 데이터를 GPU로 전송\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 출력 및 손실 계산\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./kobart-finetuned-20per/tokenizer_config.json',\n",
       " './kobart-finetuned-20per/special_tokens_map.json',\n",
       " './kobart-finetuned-20per/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./kobart-finetuned-20per')\n",
    "tokenizer.save_pretrained('./kobart-finetuned-20per')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최종 요약 결과: 함께 치는 고려시대부터 먹던 음식으로, 떡에 소시지, 치즈, 야채 등을 꽂아 구운 음식입니다. 떡꼬치는 간편하고 맛있게 즐길 수 있는 음식으로, 한국뿐만 아니라 세계적으로 인기가 있습니다.\n",
      "\n",
      "떡꼬치의 재료는 떡, 소시지, 치즈, 야채 등이 있습니다. 떡은 떡, 소시지, 치즈, 야채 등이 있습니다. 떡은 쌀가루로 만든 음식으로, 떡꼬치의 주 재료입니다. 떡꼬치의 주 재료입니다. 떡꼬치의 주 질,, 섬유질, 섬유질, 섬유질, 비타민, 비타민, 비타민, 미네랄 등이 풍부, 미네랄 등이 풍부 풍부, 미네랄 등이 풍부합니다.\n",
      "떡꼬치는 문화적 의미가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')  # KoBART 기본 모델\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2').to(device)\n",
    "\n",
    "# 텍스트 분리 함수\n",
    "def split_text(text, max_length=512):\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# 요약 함수\n",
    "def summarize(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\").to(device)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=100,  # 요약 최대 길이\n",
    "        min_length=30,   # 요약 최소 길이\n",
    "        length_penalty=1.5,  # 길이 패널티\n",
    "        num_beams=5,  # 빔 서치 사용\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 테스트 텍스트\n",
    "test_text = \"\"\"\n",
    "떡꼬치의 역사는 오래되었습니다. 떡꼬치는 고려시대부터 먹던 음식으로, 떡에 소시지, 치즈, 야채 등을 꽂아 구운 음식입니다. 떡꼬치는 간편하고 맛있게 즐길 수 있는 음식으로, 한국뿐만 아니라 세계적으로 인기가 있습니다.\n",
    "\n",
    "떡꼬치의 재료는 떡, 소시지, 치즈, 야채 등이 있습니다. 떡은 쌀가루로 만든 음식으로, 떡꼬치의 주 재료입니다. 소시지는 돼지고기, 닭고기, 쇠고기 등으로 만든 음식으로, 떡꼬치에 풍미를 더합니다. 치즈는 우유로 만든 음식으로, 떡꼬치에 고소함을 더합니다. 야채는 당근, 파, 양파, 피망 등이 있습니다. 야채는 떡꼬치에 영양을 더합니다.\n",
    "\n",
    "떡꼬치의 조리법은 간단합니다. 떡을 꼬치에 꽂고, 소시지, 치즈, 야채 등을 얹습니다. 그 다음, 떡꼬치를 오븐이나 그릴에 구워줍니다. 떡꼬치가 다 구워지면, 소금, 후추 등으로 간을 맞춥니다.\n",
    "\n",
    "떡꼬치는 맛과 특징이 있습니다. 떡꼬치는 떡의 고소함과 소시지, 치즈, 야채의 맛이 어우러져 맛있습니다. 떡꼬치는 또한 간편하고 맛있게 즐길 수 있는 음식입니다.\n",
    "\n",
    "떡꼬치는 영양성분이 있습니다. 떡꼬치는 떡, 소시지, 치즈, 야채 등이 들어있어 영양이 풍부합니다. 떡꼬치는 칼로리, 탄수화물, 단백질, 지방, 섬유질, 비타민, 미네랄 등이 풍부합니다.\n",
    "\n",
    "떡꼬치는 문화적 의미가 있습니다. 떡꼬치는 한국의 대표적인 길거리 음식으로, 한국의 문화를 대표합니다. 떡꼬치는 또한 한국의 전통과 역사를 담고 있습니다.\n",
    "\n",
    "떡꼬치는 한국의 대표적인 길거리 음식으로, 한국뿐만 아니라 세계적으로 인기가 있습니다. 떡꼬치는 간편하고 맛있게 즐길 수 있는 음식으로, 영양도 풍부합니다. 떡꼬치는 한국의 문화를 대표하는 음식으로, 한국의 전통과 역사를 담고 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 텍스트 분리 및 요약\n",
    "splitted_texts = split_text(test_text)\n",
    "summaries = [summarize(part, model, tokenizer) for part in splitted_texts]\n",
    "\n",
    "# 최종 요약 결과 출력\n",
    "final_summary = \" \".join(summaries)\n",
    "print(\"\\n최종 요약 결과:\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일로 저장 완료: training_data.csv\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# JSON 파일이 저장된 상위 디렉토리 경로\n",
    "base_dir = r'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1'  # JSON 파일이 포함된 폴더 경로\n",
    "categories = ['fm_drama', 'culture', 'enter']  # 카테고리 폴더 리스트\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "data_list = []\n",
    "\n",
    "# JSON 데이터 처리\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(base_dir, category, '3sent')  # 각 카테고리의 3sent 폴더 경로\n",
    "\n",
    "    # 3sent 폴더 안의 모든 JSON 파일 처리\n",
    "    for filename in os.listdir(category_dir):\n",
    "        file_path = os.path.join(category_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "            # 필요한 정보 추출\n",
    "            doc_type = json_data.get('Meta', {}).get('doc_type', '')\n",
    "            doc_id = json_data.get('Meta', {}).get('doc_id', '')\n",
    "            doc_origin = json_data.get('Meta', {}).get('doc_origin', '')\n",
    "            passage = json_data.get('Meta', {}).get('passage', '')\n",
    "            summary = (\n",
    "                json_data.get('Annotation', {}).get('Summary1', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary2', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary3', '')\n",
    "            )\n",
    "\n",
    "            # 리스트에 저장\n",
    "            data_list.append({\n",
    "                'type': doc_type,\n",
    "                'id': doc_id,\n",
    "                'origin': doc_origin,\n",
    "                'passage': passage,\n",
    "                'summary': summary\n",
    "            })\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = 'training_data.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"CSV 파일로 저장 완료: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일로 저장 완료: Validation_data.csv\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# JSON 파일이 저장된 상위 디렉토리 경로\n",
    "base_dir = r'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1'  # JSON 파일이 포함된 폴더 경로\n",
    "categories = ['fs_drama', 'c_event']  # 카테고리 폴더 리스트\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "data_list = []\n",
    "\n",
    "# JSON 데이터 처리\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(base_dir, category, '3sent')  # 각 카테고리의 3sent 폴더 경로\n",
    "\n",
    "    # 3sent 폴더 안의 모든 JSON 파일 처리\n",
    "    for filename in os.listdir(category_dir):\n",
    "        file_path = os.path.join(category_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "            # 필요한 정보 추출\n",
    "            doc_type = json_data.get('Meta', {}).get('doc_type', '')\n",
    "            doc_id = json_data.get('Meta', {}).get('doc_id', '')\n",
    "            doc_origin = json_data.get('Meta', {}).get('doc_origin', '')\n",
    "            passage = json_data.get('Meta', {}).get('passage', '')\n",
    "            summary = (\n",
    "                json_data.get('Annotation', {}).get('Summary1', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary2', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary3', '')\n",
    "            )\n",
    "\n",
    "            # 리스트에 저장\n",
    "            data_list.append({\n",
    "                'type': doc_type,\n",
    "                'id': doc_id,\n",
    "                'origin': doc_origin,\n",
    "                'passage': passage,\n",
    "                'summary': summary\n",
    "            })\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = 'Validation_data.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"CSV 파일로 저장 완료: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일로 저장 완료: test_data.csv\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# JSON 파일이 저장된 상위 디렉토리 경로\n",
    "base_dir = r'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1'  # JSON 파일이 포함된 폴더 경로\n",
    "categories = ['history']  # 카테고리 폴더 리스트\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "data_list = []\n",
    "\n",
    "# JSON 데이터 처리\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(base_dir, category, '3sent')  # 각 카테고리의 3sent 폴더 경로\n",
    "\n",
    "    # 3sent 폴더 안의 모든 JSON 파일 처리\n",
    "    for filename in os.listdir(category_dir):\n",
    "        file_path = os.path.join(category_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "            # 필요한 정보 추출\n",
    "            doc_type = json_data.get('Meta', {}).get('doc_type', '')\n",
    "            doc_id = json_data.get('Meta', {}).get('doc_id', '')\n",
    "            doc_origin = json_data.get('Meta', {}).get('doc_origin', '')\n",
    "            passage = json_data.get('Meta', {}).get('passage', '')\n",
    "            summary = (\n",
    "                json_data.get('Annotation', {}).get('Summary1', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary2', '') or\n",
    "                json_data.get('Annotation', {}).get('Summary3', '')\n",
    "            )\n",
    "\n",
    "            # 리스트에 저장\n",
    "            data_list.append({\n",
    "                'type': doc_type,\n",
    "                'id': doc_id,\n",
    "                'origin': doc_origin,\n",
    "                'passage': passage,\n",
    "                'summary': summary\n",
    "            })\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = 'test_data.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"CSV 파일로 저장 완료: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Destination path '/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/train.tsv' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m destination_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 파일 이동\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dongsub/lib/python3.12/shutil.py:845\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    842\u001b[0m     real_dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, _basename(src))\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(real_dst):\n\u001b[0;32m--> 845\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m real_dst)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     os\u001b[38;5;241m.\u001b[39mrename(src, real_dst)\n",
      "\u001b[0;31mError\u001b[0m: Destination path '/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/train.tsv' already exists"
     ]
    }
   ],
   "source": [
    "#csv는 구분자가 콤마(,)이고, tsv는 구분자가 탭(\\t)\n",
    "#train_data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path = '/home/metaai2/byeonguk_work/I-live-A-commerce/training_data.csv'\n",
    "df = pd.read_csv(path, sep = \",\", engine='python', encoding = \"utf-8\")\n",
    "df.dropna(axis=0)\n",
    "df.to_csv('train.tsv', sep='\\t', encoding=\"utf-8\", index=False)\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_file = '/home/metaai2/byeonguk_work/I-live-A-commerce/train.tsv'\n",
    "# 이동할 위치의 폴더 경로\n",
    "destination_folder = '/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/'\n",
    "# 파일 이동\n",
    "shutil.move(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/validation.tsv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csv는 구분자가 콤마(,)이고, tsv는 구분자가 탭(\\t)\n",
    "#train_data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path = '/home/metaai2/byeonguk_work/I-live-A-commerce/Validation_data.csv'\n",
    "df = pd.read_csv(path, sep = \",\", engine='python', encoding = \"utf-8\")\n",
    "df.dropna(axis=0)\n",
    "df.to_csv('validation.tsv', sep='\\t', encoding=\"utf-8\", index=False)\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_file = '/home/metaai2/byeonguk_work/I-live-A-commerce/validation.tsv'\n",
    "# 이동할 위치의 폴더 경로\n",
    "destination_folder = '/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/'\n",
    "# 파일 이동\n",
    "shutil.move(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/test.tsv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csv는 구분자가 콤마(,)이고, tsv는 구분자가 탭(\\t)\n",
    "#train_data\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path = '/home/metaai2/byeonguk_work/I-live-A-commerce/test_data.csv'\n",
    "df = pd.read_csv(path, sep = \",\", engine='python', encoding = \"utf-8\")\n",
    "df.dropna(axis=0)\n",
    "df.to_csv('test.tsv', sep='\\t', encoding=\"utf-8\", index=False)\n",
    "\n",
    "# 원본 파일 경로\n",
    "source_file = '/home/metaai2/byeonguk_work/I-live-A-commerce/test.tsv'\n",
    "# 이동할 위치의 폴더 경로\n",
    "destination_folder = '/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/'\n",
    "# 파일 이동\n",
    "shutil.move(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.5.1\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: numpy in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (1.26.4)\n",
      "Collecting tokenizers==0.9.3 (from transformers==3.5.1)\n",
      "  Downloading tokenizers-0.9.3.tar.gz (172 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (24.2)\n",
      "Requirement already satisfied: filelock in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (3.13.1)\n",
      "Requirement already satisfied: requests in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from transformers==3.5.1) (2024.11.6)\n",
      "Collecting sentencepiece==0.1.91 (from transformers==3.5.1)\n",
      "  Downloading sentencepiece-0.1.91.tar.gz (500 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[5 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Package sentencepiece was not found in the pkg-config search path.\n",
      "  \u001b[31m   \u001b[0m Perhaps you should add the directory containing `sentencepiece.pc'\n",
      "  \u001b[31m   \u001b[0m to the PKG_CONFIG_PATH environment variable\n",
      "  \u001b[31m   \u001b[0m No package 'sentencepiece' found\n",
      "  \u001b[31m   \u001b[0m Failed to find sentencepiece pkgconfig\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: loguru\n",
      "Successfully installed loguru-0.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning==2.0.8\n",
      "  Downloading lightning-2.0.8-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: Jinja2<5.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (3.1.3)\n",
      "Requirement already satisfied: PyYAML<8.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (6.0.2)\n",
      "Collecting arrow<3.0,>=1.2.0 (from lightning==2.0.8)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting backoff<4.0,>=2.2.1 (from lightning==2.0.8)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting beautifulsoup4<6.0,>=4.8.0 (from lightning==2.0.8)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click<10.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (8.1.8)\n",
      "Collecting croniter<1.5.0,>=1.3.0 (from lightning==2.0.8)\n",
      "  Downloading croniter-1.4.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Collecting dateutils<2.0 (from lightning==2.0.8)\n",
      "  Downloading dateutils-0.6.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting deepdiff<8.0,>=5.7.0 (from lightning==2.0.8)\n",
      "  Downloading deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<2.0,>=0.92.0 (from lightning==2.0.8)\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (2024.2.0)\n",
      "Collecting inquirer<5.0,>=2.10.0 (from lightning==2.0.8)\n",
      "  Downloading inquirer-3.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lightning-cloud>=0.5.37 (from lightning==2.0.8)\n",
      "  Downloading lightning_cloud-0.5.70-py3-none-any.whl.metadata (957 bytes)\n",
      "Collecting lightning-utilities<2.0,>=0.7.0 (from lightning==2.0.8)\n",
      "  Using cached lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (24.2)\n",
      "Requirement already satisfied: psutil<7.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (6.1.1)\n",
      "Collecting pydantic<2.2.0,>=1.7.4 (from lightning==2.0.8)\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl.metadata (136 kB)\n",
      "Collecting python-multipart<2.0,>=0.0.5 (from lightning==2.0.8)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests<4.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (2.32.3)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (13.9.4)\n",
      "Collecting starlette (from lightning==2.0.8)\n",
      "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting starsessions<2.0,>=1.2.1 (from lightning==2.0.8)\n",
      "  Downloading starsessions-1.3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (2.5.1+cu118)\n",
      "Collecting torchmetrics<2.0,>=0.7.0 (from lightning==2.0.8)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (4.67.1)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (4.12.2)\n",
      "Requirement already satisfied: urllib3<4.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning==2.0.8) (2.3.0)\n",
      "Collecting uvicorn<2.0 (from lightning==2.0.8)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websocket-client<3.0 (from lightning==2.0.8)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting websockets<13.0 (from lightning==2.0.8)\n",
      "  Downloading websockets-12.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pytorch-lightning (from lightning==2.0.8)\n",
      "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from arrow<3.0,>=1.2.0->lightning==2.0.8) (2.9.0.post0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow<3.0,>=1.2.0->lightning==2.0.8)\n",
      "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.8)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pytz in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from dateutils<2.0->lightning==2.0.8) (2024.2)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff<8.0,>=5.7.0->lightning==2.0.8)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting starlette (from lightning==2.0.8)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (3.11.11)\n",
      "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning==2.0.8)\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting editor>=1.6.0 (from inquirer<5.0,>=2.10.0->lightning==2.0.8)\n",
      "  Downloading editor-1.6.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting readchar>=4.2.0 (from inquirer<5.0,>=2.10.0->lightning==2.0.8)\n",
      "  Downloading readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from Jinja2<5.0->lightning==2.0.8) (2.1.5)\n",
      "Collecting pyjwt (from lightning-cloud>=0.5.37->lightning==2.0.8)\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: six in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning-cloud>=0.5.37->lightning==2.0.8) (1.17.0)\n",
      "Collecting boto3 (from lightning-cloud>=0.5.37->lightning==2.0.8)\n",
      "  Downloading boto3-1.36.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning-cloud>=0.5.37->lightning==2.0.8) (5.29.3)\n",
      "Requirement already satisfied: setuptools in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.7.0->lightning==2.0.8) (75.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from pydantic<2.2.0,>=1.7.4->lightning==2.0.8) (0.7.0)\n",
      "Collecting pydantic-core==2.4.0 (from pydantic<2.2.0,>=1.7.4->lightning==2.0.8)\n",
      "  Downloading pydantic_core-2.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests<4.0->lightning==2.0.8) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests<4.0->lightning==2.0.8) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from requests<4.0->lightning==2.0.8) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from rich<15.0,>=12.3.0->lightning==2.0.8) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from rich<15.0,>=12.3.0->lightning==2.0.8) (2.19.1)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette->lightning==2.0.8)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting itsdangerous<3.0.0,>=2.0.1 (from starsessions<2.0,>=1.2.1->lightning==2.0.8)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: filelock in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from sympy==1.13.1->torch<4.0,>=1.11.0->lightning==2.0.8) (1.3.0)\n",
      "Collecting h11>=0.8 (from uvicorn<2.0->lightning==2.0.8)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.8) (1.18.3)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette->lightning==2.0.8)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning==2.0.8) (0.2.13)\n",
      "Collecting runs (from editor>=1.6.0->inquirer<5.0,>=2.10.0->lightning==2.0.8)\n",
      "  Downloading runs-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xmod (from editor>=1.6.0->inquirer<5.0,>=2.10.0->lightning==2.0.8)\n",
      "  Downloading xmod-1.8.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.8) (0.1.2)\n",
      "Collecting botocore<1.37.0,>=1.36.2 (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8)\n",
      "  Downloading botocore-1.36.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8)\n",
      "  Downloading s3transfer-0.11.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading lightning-2.0.8-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
      "Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading inquirer-3.4.0-py3-none-any.whl (18 kB)\n",
      "Downloading lightning_cloud-0.5.70-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "Downloading pydantic_core-2.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-12.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Downloading editor-1.6.6-py3-none-any.whl (4.0 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Downloading readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Downloading boto3-1.36.2-py3-none-any.whl (139 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading botocore-1.36.2-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.11.1-py3-none-any.whl (84 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading runs-1.2.2-py3-none-any.whl (7.0 kB)\n",
      "Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
      "Installing collected packages: xmod, websockets, websocket-client, types-python-dateutil, soupsieve, sniffio, readchar, python-multipart, pyjwt, pydantic-core, ordered-set, lightning-utilities, jmespath, itsdangerous, h11, blessed, backoff, uvicorn, runs, pydantic, deepdiff, dateutils, croniter, botocore, beautifulsoup4, arrow, anyio, torchmetrics, starlette, s3transfer, editor, starsessions, pytorch-lightning, inquirer, fastapi, boto3, lightning-cloud, lightning\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.5\n",
      "    Uninstalling pydantic-2.10.5:\n",
      "      Successfully uninstalled pydantic-2.10.5\n",
      "Successfully installed anyio-4.8.0 arrow-1.3.0 backoff-2.2.1 beautifulsoup4-4.12.3 blessed-1.20.0 boto3-1.36.2 botocore-1.36.2 croniter-1.4.1 dateutils-0.6.12 deepdiff-7.0.1 editor-1.6.6 fastapi-0.115.6 h11-0.14.0 inquirer-3.4.0 itsdangerous-2.2.0 jmespath-1.0.1 lightning-2.0.8 lightning-cloud-0.5.70 lightning-utilities-0.11.9 ordered-set-4.1.0 pydantic-2.1.1 pydantic-core-2.4.0 pyjwt-2.10.1 python-multipart-0.0.20 pytorch-lightning-2.5.0.post0 readchar-4.2.1 runs-1.2.2 s3transfer-0.11.1 sniffio-1.3.1 soupsieve-2.6 starlette-0.41.3 starsessions-1.3.0 torchmetrics-1.6.1 types-python-dateutil-2.9.0.20241206 uvicorn-0.34.0 websocket-client-1.8.0 websockets-12.0 xmod-1.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.13.3\n",
      "  Downloading tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[49 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.5.1\n",
    "!pip install loguru\n",
    "!pip install lightning==2.0.8\n",
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb==0.15.9\n",
      "  Using cached wandb-0.15.9-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (8.1.8)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (3.1.44)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (2.32.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (6.1.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (2.20.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from wandb==0.15.9) (6.0.2)\n",
      "Collecting pathtools (from wandb==0.15.9)\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7oxopsde/pathtools_79489bf72a50404681a187e1c4d17807/setup.py\", line 25, in <module>\n",
      "  \u001b[31m   \u001b[0m     import imp\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'imp'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: tensorboard in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (75.6.0)\n",
      "Requirement already satisfied: six>1.9 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb==0.15.9\n",
    "!pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\u001b[32m2025-01-21 19:37:20.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mNamespace(train_file='/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/train.tsv', test_file='/home/metaai2/byeonguk_work/I-live-A-commerce/TL1/data/test.tsv', batch_size=4, checkpoint='checkpoint', max_len=512, max_epochs=100, lr=3e-05, accelerator='gpu', num_gpus=1, gradient_clip_val=1.0, num_workers=4)\u001b[0m\n",
      "Loaded dataset with columns: ['type', 'id', 'origin', 'passage', 'summary']\n",
      "Loaded dataset with columns: ['type', 'id', 'origin', 'passage', 'summary']\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Loaded dataset with columns: ['type', 'id', 'origin', 'passage', 'summary']\n",
      "Loaded dataset with columns: ['type', 'id', 'origin', 'passage', 'summary']\n",
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.440   Total estimated model params size (MB)\n",
      "Epoch 0:   3%|    | 150/5585 [00:22<13:18,  6.81it/s, v_num=6, train_loss=8.140]^C\n",
      "/home/metaai2/anaconda3/envs/dongsub/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python train.py --gradient_clip_val 1.0 \\\n",
    "                                       --max_epochs 50 \\\n",
    "                                       --checkpoint checkpoint \\\n",
    "                                       --accelerator gpu \\\n",
    "                                       --num_gpus 1 \\\n",
    "                                       --batch_size 4 \\\n",
    "                                       --num_workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart import BartForConditionalGeneration \n",
    "!python get_model_binary.py --model_binary '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/checkpoint/summarization_final/epoch=99-val_loss=0.000.ckpt'\n",
    "\n",
    "# 모델 바이너리 파일 경로\n",
    "model_binary_path = '/content/drive/MyDrive/KoBART-summarization-main/KoBART-summarization-main/kobart_summary'\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_binary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텍스트\n",
    "input_text =  \"\"\"\n",
    "광화문(光化門)은 서울특별시 종로구의 조선왕조 법궁인 경복궁의 남쪽에 있는 정문이다. \"임금의 큰 덕(德)이 온 나라를 비춘다\"는 의미이다. 1395년에 세워졌으며, 2층 누각 구조로 되어 있다. 경복궁의 정전인 근정전으로 가기 위해 지나야 하는 문 3개 중에서 첫째로 마주하는 문이며, 둘째는 흥례문, 셋째는 근정문이다. 광화문 앞에는 지금은 도로 건설로 사라진 월대가 자리잡고 있었으며, 양쪽에는 한 쌍의 해태 조각상이 자리잡고 있다. 광화문의 석축부에는 세 개의 홍예문(虹霓門, 아치문)이 있다. 가운데 문은 임금이 다니던 문이고, 나머지 좌우의 문은 신하들이 다니던 문이었는데, 왼쪽 문은 무신이, 오른쪽 문은 문신이 출입했다. 광화문의 가운데 문 천장에는 주작이 그려져 있고, 왼쪽 문에는 거북이가, 오른쪽 문에는 천마가 그려져 있다.\n",
    "\"\"\"\n",
    "# 입력 텍스트를 토큰화하여 인코딩\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 모델을 사용하여 요약 생성\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 요약 결과 디코딩\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 요약 출력\n",
    "print(\"요약 텍스트:\", summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongsub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
